{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d2a7499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device : mps\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import typing as tt\n",
    "import torch  \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "from gymnasium.wrappers.vector import RecordEpisodeStatistics\n",
    "\n",
    "HIDDEN_LAYER1  = 256\n",
    "# ALPHA = 0.95\n",
    "GAMMA = 0.95 # DISCOUNT FACTOR\n",
    "LAMBDA = 0.95 # FOR GAE\n",
    "LR = 3e-4\n",
    "# N_STEPS = 20\n",
    "ENV_ID = 'InvertedPendulum-v5'\n",
    "N_ENVS = 3\n",
    "N_STEPS = 5\n",
    "BATCH_SIZE = N_ENVS * N_STEPS\n",
    "\n",
    "ENTROPY_BETA = 0.01\n",
    "ENTROPY_BETA_MIN = 1e-5\n",
    "entropy_smoothing_factor = 0.05\n",
    "total_updates = 500000 // BATCH_SIZE\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu' \n",
    "print(f'Using device : {device}')\n",
    "\n",
    "\n",
    "# env = gym.make(ENV_ID)\n",
    "envs = gym.make_vec(ENV_ID, num_envs=N_ENVS, vectorization_mode='async' )\n",
    "envs = RecordEpisodeStatistics(envs) #handles reward logging\n",
    "eval_env = gym.make(ENV_ID, render_mode='rgb_array')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def smooth(old: tt.Optional[float], val: float, alpha: float = 0.95,) -> float:\n",
    "    if old is None:\n",
    "        return val\n",
    "    return old * alpha + (1-alpha)*val    \n",
    "\n",
    "def record_video(env, policy, device, low, high, max_steps=500, ):\n",
    "    \"\"\"Record a single episode and return frames + reward\"\"\"\n",
    "    frames = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    frame = env.render()\n",
    "    if frame is not None:\n",
    "        frames.append(np.array(frame, copy=True))\n",
    "    while not done and steps < max_steps:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            mu, std, _ = policy(state_tensor)\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        action = torch.clamp(dist.sample(), low, high)\n",
    "\n",
    "        state, reward, terminated, truncated, _ = env.step(action.squeeze(0).cpu().numpy())\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        steps += 1\n",
    "\n",
    "        frame = env.render()\n",
    "        if frame is not None:\n",
    "            frames.append(np.array(frame, copy=True))\n",
    "        \n",
    "    return frames, total_reward, steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_size, fc, action_dim, log_std_min, log_std_max):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc = fc\n",
    "        self.action_dim = action_dim\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.fc), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(self.fc, self.fc), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mu = nn.Linear(self.fc, self.action_dim)\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.zeros(self.action_dim))\n",
    "        \n",
    "        self.critic_head = nn.Linear(self.fc, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        std = torch.exp(torch.clamp(self.log_std, self.log_std_min, self.log_std_max))\n",
    "        std = std.expand_as(mu)\n",
    "        \n",
    "        val = self.critic_head(x)\n",
    "        return mu, std, val\n",
    "    \n",
    "class LinearBetaScheduler:\n",
    "    def __init__(self, beta_start, beta_end, total_steps):\n",
    "        self.start = beta_start\n",
    "        self.end = beta_end\n",
    "        self.total_steps = total_steps\n",
    "\n",
    "    def update(self, current_step):\n",
    "        # Linearly decay beta based on step count\n",
    "        frac = min(1.0, current_step / self.total_steps)\n",
    "        return self.start + frac * (self.end - self.start)\n",
    "    \n",
    "class BetaScheduler:\n",
    "    def __init__(self, target_reward, beta_start, beta_min=1e-4, smoothing_factor=0.01):\n",
    "        self.target = target_reward\n",
    "        self.start = beta_start\n",
    "        self.min = beta_min\n",
    "        self.alpha = smoothing_factor\n",
    "        self.ema_reward = None  # Exponential Moving Average of Reward\n",
    "        self.current_beta = beta_start\n",
    "\n",
    "    def update(self, reward):\n",
    "        # 1. Update EMA of Reward\n",
    "        if self.ema_reward is None:\n",
    "            self.ema_reward = reward\n",
    "        else:\n",
    "            self.ema_reward = (self.ema_reward * (1 - self.alpha)) + (reward * self.alpha)\n",
    "        \n",
    "        # 2. Calculate Progress (0.0 to 1.0) based on EMA\n",
    "        # If ema_reward is negative, treat progress as 0\n",
    "        progress = max(0.0, min(1.0, self.ema_reward / self.target))\n",
    "        \n",
    "        # 3. Decay Beta linearly with progress\n",
    "        self.current_beta = self.start * (1.0 - progress)\n",
    "        \n",
    "        # 4. Clamp to minimum\n",
    "        self.current_beta = max(self.current_beta, self.min)\n",
    "        \n",
    "        return self.current_beta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c4ade1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, next_values, dones, gamma, lam):\n",
    "    \n",
    "    \n",
    "    # print(f\"REWARDS:{rewards}\")\n",
    "    # print(f'DONES: {dones}')\n",
    "    \n",
    "    mask = 1.0 - dones\n",
    "    # print('MASK', mask)\n",
    "\n",
    "    delta_t = rewards + (gamma * mask * next_values) - values\n",
    "    \n",
    "    T = delta_t.shape[0]\n",
    "    adv = torch.zeros_like(delta_t)\n",
    "    gae = 0.0\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        gae = delta_t[t] + gamma * lam * mask[t] * gae\n",
    "        adv[t] = gae\n",
    "\n",
    "    return adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a6d3a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dtype\n",
    "\n",
    "\n",
    "class VectorCollector:\n",
    "    def __init__(self, envs, policy, gamma, lam, n_steps,action_low, action_high,  device):\n",
    "        # super().__init__(self,)\n",
    "        self.env = envs\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.n_steps = n_steps\n",
    "        self.device = device\n",
    "        \n",
    "        self.ep_reward = 0\n",
    "        \n",
    "        self.state, _ = envs.reset()\n",
    "        print(self.state)\n",
    "        self.action_bias = (action_high + action_low) / 2\n",
    "        self.action_scale = (action_high - action_low) / 2\n",
    "                \n",
    "        self.states = []\n",
    "        self.raw_actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "        self.deltas = []\n",
    "        self.values = []         \n",
    "          \n",
    "    def rollout(self):\n",
    "        \n",
    "        while True:\n",
    "            batch_states = []\n",
    "            batch_actions = []\n",
    "            batch_rewards = []\n",
    "            batch_dones = []\n",
    "            batch_values = []\n",
    "            batch_deltas = []\n",
    "            \n",
    "            episode_rewards = []\n",
    "            batch_value_next = []\n",
    "            \n",
    "            for _ in range(self.n_steps):\n",
    "                # print(f\"state: {self.state}\")\n",
    "                \n",
    "                state_t = torch.tensor(self.state, dtype=torch.float32, device=device)#.unsqueeze(0)\n",
    "                # print(state_t)\n",
    "                with torch.no_grad():\n",
    "                    mu, std, val = self.policy(state_t)\n",
    "                # print(f\"mu: {mu}\")\n",
    "                # print(f'std: {std}')\n",
    "                # print(f\"val: {val}\")\n",
    "                # return\n",
    "                # # print('mu', mu)\n",
    "                # # print('std', std)\n",
    "                dist = torch.distributions.Normal(mu,std)\n",
    "                u = dist.sample()\n",
    "                a = torch.tanh(u)\n",
    "                \n",
    "                action = a*self.action_scale + self.action_bias\n",
    "                # print(f\"action:{action}\")\n",
    "                action_env = action.detach().cpu().numpy()\n",
    "                # action_env = action.squeeze(0).detach().cpu().numpy()\n",
    "                # print(f\"action env:{action_env}\")\n",
    "                # return\n",
    "                next_state, rew, term, trunc, info = self.env.step(action_env)\n",
    "                # self.next_state_t = torch.Tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                done = term | trunc\n",
    "                done_t = torch.tensor(done, dtype=torch.float32, device=self.device)\n",
    "                rew_t = torch.tensor(rew, dtype=torch.float32, device=self.device)\n",
    "                # print(f'next_state: {next_state}')\n",
    "                # print(f\"done: {done}\")\n",
    "                # print(f\"rew: {rew}\")\n",
    "                # print(f\"info: {info}\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                batch_states.append(state_t)\n",
    "                batch_actions.append(u)\n",
    "                batch_rewards.append(torch.tensor(rew, dtype=torch.float32, device=self.device)) # rew is converted to tensor to seperate each n_steps\n",
    "                batch_dones.append(torch.tensor(rew, dtype=torch.float32, device=self.device))\n",
    "                batch_values.append(val.squeeze(dim=-1))\n",
    "                # print(f\"batch_actions: {batch_actions}\")\n",
    "                # print(f\"batch_states: {batch_states}\")\n",
    "                # print(f\"batch_rewards: {batch_rewards}\")\n",
    "                # print(f\"batch_dones: {batch_dones}\")\n",
    "                # print(f\"batch_values : {batch_values}\")\n",
    "                # yield None\n",
    "                # continue\n",
    "                if '_episode' in info:\n",
    "                    for idx, has_ep in enumerate(info['_episode']):\n",
    "                        if has_ep:\n",
    "                            if 'episode' in info:\n",
    "                                # print(f'idx: {idx}')\n",
    "                                # print(f\"episode r: {info['episode']['r']}\")\n",
    "                                episode_rewards.append(info['episode']['r'][idx])\n",
    "                else: \n",
    "                    episode_rewards = []\n",
    "                # print(f'{episode_rewards}')\n",
    "                \n",
    "                self.state = next_state\n",
    "\n",
    "                # yield None\n",
    "                # continue\n",
    "                \n",
    "                \n",
    "                \n",
    "            # bootstrapping\n",
    "            with torch.no_grad():\n",
    "                next_state_t = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "                _, _, nxt_val = self.policy(next_state_t)\n",
    "                # print(f'next_val before squeeze dim=-1:{nxt_val}')\n",
    "                nxt_val = nxt_val.squeeze(dim=-1)\n",
    "                # print(f'next_val after squeeze dim=-1:{nxt_val}')\n",
    "                \n",
    "            T_rewards = torch.stack(batch_rewards, dim=0)\n",
    "            T_dones = torch.stack(batch_dones, dim=0)\n",
    "            T_values = torch.stack(batch_values, dim=0)\n",
    "            \n",
    "            # print(f\"values_t : {T_values}, {T_values.shape}\")\n",
    "            # print(f'next values after unsqueeze: {nxt_val}, {nxt_val.unsqueeze(dim=0).shape}')\n",
    "            T_values_next = torch.cat((T_values[1:], nxt_val.unsqueeze(dim=0)), dim=0)\n",
    "            # print(f\"values_t : {T_values_next}, {T_values_next.shape}\")\n",
    "        \n",
    "            batch_adv = compute_gae(rewards=T_rewards, \n",
    "                                    values=T_values,  \n",
    "                                    next_values=T_values_next, \n",
    "                                    dones=T_dones, \n",
    "                                    gamma=self.gamma, \n",
    "                                    lam=self.lam )\n",
    "            \n",
    "            batch_returns = batch_adv + T_values\n",
    "            # print(f'batch adv: {batch_adv}')\n",
    "            # print(f'batch_returns: {batch_returns}')\n",
    "            \n",
    "            yield {\n",
    "                    'states':batch_states, \n",
    "                    'actions':batch_actions, \n",
    "                    'done':batch_dones, \n",
    "                    'adv':batch_adv,\n",
    "                    'ep_rewards': episode_rewards,\n",
    "                    'values':batch_values, \n",
    "                    'returns':batch_returns\n",
    "            }\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c0f05f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00717337 -0.00365154 -0.00430585  0.00299112]\n",
      " [ 0.00333091 -0.00306471 -0.00089421 -0.00801352]\n",
      " [-0.0008977  -0.0073802   0.0037997   0.00902852]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "policy = PolicyNet(\n",
    "    input_size=envs.single_observation_space.shape[0], \n",
    "    fc = HIDDEN_LAYER1, \n",
    "    action_dim=envs.single_action_space.shape[0], \n",
    "    log_std_min=-20, \n",
    "    log_std_max=1,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr = LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='max', \n",
    "    factor=0.5, \n",
    "    patience=500,  # If reward doesn't go up for 500 steps, lower LR\n",
    ")\n",
    "\n",
    "current_beta = ENTROPY_BETA\n",
    "beta_scheduler = LinearBetaScheduler(\n",
    "    beta_start=ENTROPY_BETA, \n",
    "    beta_end=ENTROPY_BETA_MIN, \n",
    "    total_steps=total_updates   # Decay fully in the first 33% of training\n",
    ")\n",
    "# beta_scheduler = BetaScheduler(\n",
    "#     target_reward=950, \n",
    "#     beta_start=ENTROPY_BETA, \n",
    "#     beta_min=ENTROPY_BETA_MIN, \n",
    "#     smoothing_factor=entropy_smoothing_factor\n",
    "# )\n",
    "\n",
    "action_low = torch.tensor(envs.single_action_space.low, dtype=torch.float32, device=device)\n",
    "action_high = torch.tensor(envs.single_action_space.high, dtype=torch.float32, device=device)\n",
    "exp_collector = VectorCollector(envs, policy, GAMMA, LAMBDA, N_STEPS,action_low, action_high, device)\n",
    "total_rewards = []\n",
    "episode_idx = 0\n",
    "mu_old = 0\n",
    "adv_smoothed = None\n",
    "l_entropy = None\n",
    "l_policy = None\n",
    "l_value = None\n",
    "l_total = None\n",
    "mean_reward = 0.0\n",
    "solved = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cdae5a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 0 |Steps: 10 | Reward: 9.0 | Mean: 9.0\n",
      "Episode: 1 |Steps: 10 | Reward: 5.0 | Mean: 7.0\n",
      "\n",
      "Episode: 2 |Steps: 11 | Reward: 3.0 | Mean: 5.7\n",
      "\n",
      "Episode: 3 |Steps: 12 | Reward: 4.0 | Mean: 5.2\n",
      "Episode: 4 |Steps: 12 | Reward: 3.0 | Mean: 4.8\n",
      "Episode: 5 |Steps: 12 | Reward: 5.0 | Mean: 4.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 6 |Steps: 19 | Reward: 3.0 | Mean: 4.6\n",
      "Episode: 7 |Steps: 19 | Reward: 6.0 | Mean: 4.8\n",
      "\n",
      "\n",
      "Episode: 8 |Steps: 21 | Reward: 8.0 | Mean: 5.1\n",
      "Episode: 9 |Steps: 21 | Reward: 2.0 | Mean: 4.8\n",
      "\n",
      "Episode: 10 |Steps: 22 | Reward: 3.0 | Mean: 4.6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 11 |Steps: 27 | Reward: 11.0 | Mean: 5.2\n",
      "\n",
      "Episode: 12 |Steps: 28 | Reward: 14.0 | Mean: 5.8\n",
      "Episode: 13 |Steps: 28 | Reward: 9.0 | Mean: 6.1\n",
      "\n",
      "Episode: 14 |Steps: 29 | Reward: 3.0 | Mean: 5.9\n",
      "Episode: 15 |Steps: 29 | Reward: 8.0 | Mean: 6.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 16 |Steps: 34 | Reward: 5.0 | Mean: 5.9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 17 |Steps: 39 | Reward: 4.0 | Mean: 5.8\n",
      "Episode: 18 |Steps: 39 | Reward: 11.0 | Mean: 6.1\n",
      "\n",
      "\n",
      "Episode: 19 |Steps: 41 | Reward: 2.0 | Mean: 5.9\n",
      "\n",
      "Episode: 20 |Steps: 42 | Reward: 2.0 | Mean: 5.7\n",
      "Episode: 21 |Steps: 42 | Reward: 2.0 | Mean: 5.5\n",
      "\n",
      "Episode: 22 |Steps: 43 | Reward: 3.0 | Mean: 5.4\n",
      "\n",
      "\n",
      "\n",
      "Episode: 23 |Steps: 46 | Reward: 2.0 | Mean: 5.3\n",
      "\n",
      "Episode: 24 |Steps: 47 | Reward: 5.0 | Mean: 5.3\n",
      "Episode: 25 |Steps: 47 | Reward: 3.0 | Mean: 5.2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 26 |Steps: 51 | Reward: 2.0 | Mean: 5.1\n",
      "\n",
      "Episode: 27 |Steps: 52 | Reward: 3.0 | Mean: 5.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 28 |Steps: 57 | Reward: 6.0 | Mean: 5.0\n",
      "Episode: 29 |Steps: 57 | Reward: 7.0 | Mean: 5.1\n",
      "\n",
      "\n",
      "\n",
      "Episode: 30 |Steps: 60 | Reward: 4.0 | Mean: 5.1\n",
      "Episode: 31 |Steps: 60 | Reward: 2.0 | Mean: 5.0\n",
      "\n",
      "Episode: 32 |Steps: 61 | Reward: 3.0 | Mean: 4.9\n",
      "Episode: 33 |Steps: 61 | Reward: 5.0 | Mean: 4.9\n",
      "Episode: 34 |Steps: 61 | Reward: 3.0 | Mean: 4.9\n",
      "\n",
      "Episode: 35 |Steps: 62 | Reward: 2.0 | Mean: 4.8\n",
      "Episode: 36 |Steps: 62 | Reward: 3.0 | Mean: 4.7\n",
      "Episode: 37 |Steps: 62 | Reward: 4.0 | Mean: 4.7\n",
      "\n",
      "Episode: 38 |Steps: 63 | Reward: 3.0 | Mean: 4.7\n",
      "Episode: 39 |Steps: 63 | Reward: 4.0 | Mean: 4.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 40 |Steps: 67 | Reward: 2.0 | Mean: 4.6\n",
      "\n",
      "Episode: 41 |Steps: 68 | Reward: 5.0 | Mean: 4.6\n",
      "\n",
      "Episode: 42 |Steps: 69 | Reward: 2.0 | Mean: 4.5\n",
      "\n",
      "\n",
      "Episode: 43 |Steps: 71 | Reward: 2.0 | Mean: 4.5\n",
      "\n",
      "\n",
      "Episode: 44 |Steps: 73 | Reward: 2.0 | Mean: 4.4\n",
      "\n",
      "Episode: 45 |Steps: 74 | Reward: 5.0 | Mean: 4.4\n",
      "\n",
      "Episode: 46 |Steps: 75 | Reward: 3.0 | Mean: 4.4\n",
      "Episode: 47 |Steps: 75 | Reward: 2.0 | Mean: 4.4\n",
      "Episode: 48 |Steps: 75 | Reward: 8.0 | Mean: 4.4\n",
      "\n",
      "\n",
      "Episode: 49 |Steps: 77 | Reward: 9.0 | Mean: 4.5\n",
      "Episode: 50 |Steps: 77 | Reward: 8.0 | Mean: 4.6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 51 |Steps: 81 | Reward: 9.0 | Mean: 4.7\n",
      "Episode: 52 |Steps: 81 | Reward: 8.0 | Mean: 4.7\n",
      "Episode: 53 |Steps: 81 | Reward: 2.0 | Mean: 4.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 54 |Steps: 87 | Reward: 2.0 | Mean: 4.6\n",
      "\n",
      "\n",
      "Episode: 55 |Steps: 89 | Reward: 3.0 | Mean: 4.6\n",
      "Episode: 56 |Steps: 89 | Reward: 4.0 | Mean: 4.6\n",
      "\n",
      "Episode: 57 |Steps: 90 | Reward: 3.0 | Mean: 4.6\n",
      "Episode: 58 |Steps: 90 | Reward: 3.0 | Mean: 4.5\n",
      "\n",
      "Episode: 59 |Steps: 91 | Reward: 3.0 | Mean: 4.5\n",
      "\n",
      "\n",
      "Episode: 60 |Steps: 93 | Reward: 7.0 | Mean: 4.6\n",
      "Episode: 61 |Steps: 93 | Reward: 6.0 | Mean: 4.6\n",
      "\n",
      "\n",
      "Episode: 62 |Steps: 95 | Reward: 8.0 | Mean: 4.6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 63 |Steps: 99 | Reward: 10.0 | Mean: 4.7\n",
      "\n",
      "\n",
      "\n",
      "Episode: 64 |Steps: 102 | Reward: 8.0 | Mean: 4.8\n",
      "Episode: 65 |Steps: 102 | Reward: 9.0 | Mean: 4.8\n",
      "\n",
      "\n",
      "Episode: 66 |Steps: 104 | Reward: 6.0 | Mean: 4.9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 67 |Steps: 108 | Reward: 7.0 | Mean: 4.9\n",
      "\n",
      "\n",
      "Episode: 68 |Steps: 110 | Reward: 7.0 | Mean: 4.9\n",
      "\n",
      "\n",
      "Episode: 69 |Steps: 112 | Reward: 8.0 | Mean: 5.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 70 |Steps: 119 | Reward: 4.0 | Mean: 4.9\n",
      "\n",
      "\n",
      "Episode: 71 |Steps: 121 | Reward: 6.0 | Mean: 5.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 72 |Steps: 125 | Reward: 21.0 | Mean: 5.2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 73 |Steps: 135 | Reward: 20.0 | Mean: 5.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 74 |Steps: 140 | Reward: 10.0 | Mean: 5.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 75 |Steps: 148 | Reward: 11.0 | Mean: 5.5\n",
      "\n",
      "\n",
      "Episode: 76 |Steps: 150 | Reward: 3.0 | Mean: 5.5\n",
      "Episode: 77 |Steps: 150 | Reward: 21.0 | Mean: 5.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 78 |Steps: 158 | Reward: 7.0 | Mean: 5.7\n",
      "\n",
      "Episode: 79 |Steps: 159 | Reward: 3.0 | Mean: 5.7\n",
      "\n",
      "\n",
      "\n",
      "Episode: 80 |Steps: 162 | Reward: 4.0 | Mean: 5.6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 81 |Steps: 203 | Reward: 14.0 | Mean: 5.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 82 |Steps: 212 | Reward: 7.0 | Mean: 5.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 83 |Steps: 219 | Reward: 6.0 | Mean: 5.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 84 |Steps: 239 | Reward: 10.0 | Mean: 5.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 85 |Steps: 245 | Reward: 5.0 | Mean: 5.8\n",
      "\n",
      "Episode: 86 |Steps: 246 | Reward: 17.0 | Mean: 5.9\n",
      "Episode: 87 |Steps: 246 | Reward: 30.0 | Mean: 6.2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 88 |Steps: 255 | Reward: 7.0 | Mean: 6.2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 89 |Steps: 259 | Reward: 34.0 | Mean: 6.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 90 |Steps: 267 | Reward: 19.0 | Mean: 6.7\n",
      "\n",
      "\n",
      "\n",
      "Episode: 91 |Steps: 270 | Reward: 15.0 | Mean: 6.8\n",
      "\n",
      "Episode: 92 |Steps: 271 | Reward: 25.0 | Mean: 6.9\n",
      "\n",
      "Episode: 93 |Steps: 272 | Reward: 6.0 | Mean: 6.9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 94 |Steps: 277 | Reward: 23.0 | Mean: 7.1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 95 |Steps: 284 | Reward: 20.0 | Mean: 7.2\n",
      "Episode: 96 |Steps: 284 | Reward: 20.0 | Mean: 7.4\n",
      "\n",
      "\n",
      "Episode: 97 |Steps: 286 | Reward: 32.0 | Mean: 7.6\n",
      "\n",
      "\n",
      "\n",
      "Episode: 98 |Steps: 289 | Reward: 13.0 | Mean: 7.7\n",
      "\n",
      "\n",
      "\n",
      "Episode: 99 |Steps: 292 | Reward: 12.0 | Mean: 7.7\n",
      "Episode: 100 |Steps: 292 | Reward: 7.0 | Mean: 7.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 101 |Steps: 300 | Reward: 38.0 | Mean: 8.0\n",
      "Episode: 102 |Steps: 300 | Reward: 25.0 | Mean: 8.2\n",
      "Episode: 103 |Steps: 300 | Reward: 19.0 | Mean: 8.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 104 |Steps: 311 | Reward: 29.0 | Mean: 8.7\n",
      "\n",
      "\n",
      "\n",
      "Episode: 105 |Steps: 314 | Reward: 7.0 | Mean: 8.7\n",
      "\n",
      "\n",
      "\n",
      "Episode: 106 |Steps: 317 | Reward: 13.0 | Mean: 8.8\n",
      "\n",
      "\n",
      "\n",
      "Episode: 107 |Steps: 320 | Reward: 42.0 | Mean: 9.1\n",
      "Episode: 108 |Steps: 320 | Reward: 15.0 | Mean: 9.2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 109 |Steps: 326 | Reward: 6.0 | Mean: 9.2\n",
      "\n",
      "\n",
      "Episode: 110 |Steps: 328 | Reward: 8.0 | Mean: 9.3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 111 |Steps: 335 | Reward: 33.0 | Mean: 9.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 112 |Steps: 352 | Reward: 56.0 | Mean: 9.9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 113 |Steps: 363 | Reward: 11.0 | Mean: 10.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 114 |Steps: 367 | Reward: 73.0 | Mean: 10.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 115 |Steps: 387 | Reward: 62.0 | Mean: 11.2\n",
      "Episode: 116 |Steps: 387 | Reward: 44.0 | Mean: 11.6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 117 |Steps: 399 | Reward: 30.0 | Mean: 11.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 118 |Steps: 403 | Reward: 54.0 | Mean: 12.3\n",
      "\n",
      "\n",
      "\n",
      "Episode: 119 |Steps: 406 | Reward: 13.0 | Mean: 12.4\n",
      "\n",
      "\n",
      "Episode: 120 |Steps: 408 | Reward: 36.0 | Mean: 12.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 121 |Steps: 416 | Reward: 14.0 | Mean: 12.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 122 |Steps: 422 | Reward: 40.0 | Mean: 13.2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 123 |Steps: 462 | Reward: 59.0 | Mean: 13.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 124 |Steps: 474 | Reward: 27.0 | Mean: 14.0\n",
      "\n",
      "\n",
      "\n",
      "Episode: 125 |Steps: 477 | Reward: 7.0 | Mean: 14.1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 126 |Steps: 494 | Reward: 15.0 | Mean: 14.2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 127 |Steps: 500 | Reward: 5.0 | Mean: 14.2\n",
      "\n",
      "Episode: 128 |Steps: 501 | Reward: 41.0 | Mean: 14.6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 129 |Steps: 506 | Reward: 9.0 | Mean: 14.6\n",
      "\n",
      "Episode: 130 |Steps: 507 | Reward: 20.0 | Mean: 14.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 131 |Steps: 532 | Reward: 11.0 | Mean: 14.8\n",
      "Episode: 132 |Steps: 532 | Reward: 27.0 | Mean: 15.1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 133 |Steps: 537 | Reward: 23.0 | Mean: 15.2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 134 |Steps: 548 | Reward: 7.0 | Mean: 15.3\n",
      "\n",
      "\n",
      "Episode: 135 |Steps: 550 | Reward: 16.0 | Mean: 15.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 136 |Steps: 560 | Reward: 31.0 | Mean: 15.7\n",
      "Episode: 137 |Steps: 560 | Reward: 34.0 | Mean: 16.0\n",
      "\n",
      "\n",
      "Episode: 138 |Steps: 562 | Reward: 42.0 | Mean: 16.4\n",
      "\n",
      "Episode: 139 |Steps: 563 | Reward: 13.0 | Mean: 16.5\n",
      "Episode: 140 |Steps: 563 | Reward: 13.0 | Mean: 16.6\n",
      "\n",
      "\n",
      "Episode: 141 |Steps: 565 | Reward: 8.0 | Mean: 16.6\n",
      "\n",
      "\n",
      "\n",
      "Episode: 142 |Steps: 568 | Reward: 24.0 | Mean: 16.8\n",
      "\n",
      "\n",
      "\n",
      "Episode: 143 |Steps: 571 | Reward: 13.0 | Mean: 16.9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 144 |Steps: 583 | Reward: 6.0 | Mean: 17.0\n",
      "Episode: 145 |Steps: 583 | Reward: 12.0 | Mean: 17.1\n",
      "\n",
      "\n",
      "\n",
      "Episode: 146 |Steps: 586 | Reward: 13.0 | Mean: 17.2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 147 |Steps: 597 | Reward: 6.0 | Mean: 17.2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 148 |Steps: 603 | Reward: 6.0 | Mean: 17.2\n",
      "\n",
      "\n",
      "\n",
      "Episode: 149 |Steps: 606 | Reward: 24.0 | Mean: 17.3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 150 |Steps: 612 | Reward: 10.0 | Mean: 17.4\n",
      "Episode: 151 |Steps: 612 | Reward: 7.0 | Mean: 17.3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 152 |Steps: 617 | Reward: 16.0 | Mean: 17.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 153 |Steps: 621 | Reward: 12.0 | Mean: 17.5\n",
      "Episode: 154 |Steps: 621 | Reward: 18.0 | Mean: 17.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 155 |Steps: 640 | Reward: 30.0 | Mean: 17.9\n",
      "Episode: 156 |Steps: 640 | Reward: 25.0 | Mean: 18.1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 157 |Steps: 648 | Reward: 14.0 | Mean: 18.3\n",
      "\n",
      "\n",
      "Episode: 158 |Steps: 650 | Reward: 26.0 | Mean: 18.5\n",
      "\n",
      "\n",
      "\n",
      "Episode: 159 |Steps: 653 | Reward: 23.0 | Mean: 18.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 160 |Steps: 661 | Reward: 13.0 | Mean: 18.8\n",
      "Episode: 161 |Steps: 661 | Reward: 50.0 | Mean: 19.2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 162 |Steps: 679 | Reward: 89.0 | Mean: 20.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 163 |Steps: 693 | Reward: 36.0 | Mean: 20.3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 164 |Steps: 709 | Reward: 16.0 | Mean: 20.3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 165 |Steps: 729 | Reward: 22.0 | Mean: 20.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 166 |Steps: 746 | Reward: 26.0 | Mean: 20.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 167 |Steps: 767 | Reward: 30.0 | Mean: 20.9\n",
      "Episode: 168 |Steps: 767 | Reward: 50.0 | Mean: 21.3\n",
      "\n",
      "Episode: 169 |Steps: 768 | Reward: 11.0 | Mean: 21.4\n",
      "\n",
      "\n",
      "Episode: 170 |Steps: 770 | Reward: 8.0 | Mean: 21.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 171 |Steps: 775 | Reward: 5.0 | Mean: 21.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 172 |Steps: 805 | Reward: 26.0 | Mean: 21.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 173 |Steps: 809 | Reward: 21.0 | Mean: 21.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 174 |Steps: 821 | Reward: 54.0 | Mean: 21.9\n",
      "\n",
      "Episode: 175 |Steps: 822 | Reward: 63.0 | Mean: 22.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 176 |Steps: 844 | Reward: 55.0 | Mean: 22.9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 177 |Steps: 864 | Reward: 4.0 | Mean: 22.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 178 |Steps: 869 | Reward: 29.0 | Mean: 23.0\n",
      "Episode: 179 |Steps: 869 | Reward: 11.0 | Mean: 23.1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 180 |Steps: 876 | Reward: 7.0 | Mean: 23.1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 181 |Steps: 884 | Reward: 46.0 | Mean: 23.4\n",
      "\n",
      "\n",
      "\n",
      "Episode: 182 |Steps: 887 | Reward: 27.0 | Mean: 23.6\n",
      "\n",
      "\n",
      "Episode: 183 |Steps: 889 | Reward: 8.0 | Mean: 23.6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 184 |Steps: 893 | Reward: 9.0 | Mean: 23.6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 185 |Steps: 901 | Reward: 26.0 | Mean: 23.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 186 |Steps: 929 | Reward: 17.0 | Mean: 23.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 187 |Steps: 965 | Reward: 5.0 | Mean: 23.6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 188 |Steps: 969 | Reward: 71.0 | Mean: 24.2\n",
      "\n",
      "Episode: 189 |Steps: 970 | Reward: 23.0 | Mean: 24.1\n",
      "\n",
      "Episode: 190 |Steps: 971 | Reward: 8.0 | Mean: 24.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 191 |Steps: 980 | Reward: 43.0 | Mean: 24.3\n",
      "\n",
      "\n",
      "\n",
      "Episode: 192 |Steps: 983 | Reward: 11.0 | Mean: 24.1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 193 |Steps: 1015 | Reward: 16.0 | Mean: 24.2\n",
      "Episode: 194 |Steps: 1015 | Reward: 15.0 | Mean: 24.2\n",
      "Episode: 195 |Steps: 1015 | Reward: 55.0 | Mean: 24.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 196 |Steps: 1068 | Reward: 29.0 | Mean: 24.6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 197 |Steps: 1080 | Reward: 16.0 | Mean: 24.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 198 |Steps: 1088 | Reward: 19.0 | Mean: 24.5\n",
      "\n",
      "\n",
      "Episode: 199 |Steps: 1090 | Reward: 25.0 | Mean: 24.6\n",
      "Episode: 200 |Steps: 1090 | Reward: 7.0 | Mean: 24.6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 201 |Steps: 1094 | Reward: 18.0 | Mean: 24.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 202 |Steps: 1101 | Reward: 46.0 | Mean: 24.6\n",
      "\n",
      "\n",
      "\n",
      "Episode: 203 |Steps: 1104 | Reward: 39.0 | Mean: 24.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 204 |Steps: 1136 | Reward: 19.0 | Mean: 24.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 205 |Steps: 1141 | Reward: 37.0 | Mean: 25.0\n",
      "\n",
      "\n",
      "Episode: 206 |Steps: 1143 | Reward: 14.0 | Mean: 25.1\n",
      "\n",
      "\n",
      "\n",
      "Episode: 207 |Steps: 1146 | Reward: 48.0 | Mean: 25.1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 208 |Steps: 1158 | Reward: 73.0 | Mean: 25.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 209 |Steps: 1216 | Reward: 75.0 | Mean: 26.4\n",
      "\n",
      "\n",
      "\n",
      "Episode: 210 |Steps: 1219 | Reward: 72.0 | Mean: 27.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 211 |Steps: 1227 | Reward: 38.0 | Mean: 27.1\n",
      "\n",
      "\n",
      "Episode: 212 |Steps: 1229 | Reward: 15.0 | Mean: 26.7\n",
      "\n",
      "\n",
      "Episode: 213 |Steps: 1231 | Reward: 18.0 | Mean: 26.7\n",
      "Episode: 214 |Steps: 1231 | Reward: 17.0 | Mean: 26.2\n",
      "\n",
      "Episode: 215 |Steps: 1232 | Reward: 13.0 | Mean: 25.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 216 |Steps: 1243 | Reward: 12.0 | Mean: 25.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 217 |Steps: 1248 | Reward: 34.0 | Mean: 25.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 218 |Steps: 1254 | Reward: 11.0 | Mean: 25.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 219 |Steps: 1260 | Reward: 10.0 | Mean: 24.9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episode: 220 |Steps: 1264 | Reward: 12.0 | Mean: 24.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     86\u001b[39m loss_total = loss_value + loss_policy - current_beta*entropy\n\u001b[32m     88\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[43mloss_total\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=\u001b[32m0.5\u001b[39m)\n\u001b[32m     91\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/drl_env/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/drl_env/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/drl_env/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(\"Recording initial video (before training)...\")\n",
    "# initial_frames, initial_reward, initial_steps = record_video(eval_env, policy, device, low = action_low, high = action_high)\n",
    "# wandb.log({\n",
    "#     \"video\": wandb.Video(\n",
    "#         np.array(initial_frames).transpose(0, 3, 1, 2), \n",
    "#         fps=30, \n",
    "#         format=\"mp4\",\n",
    "#         caption=f\"Initial (untrained) - Reward: {initial_reward}, Steps: {initial_steps}\"\n",
    "#     ),\n",
    "#     \"initial_reward\": initial_reward\n",
    "# }, step=0)\n",
    "# print(f\"Initial reward: {initial_reward}, steps: {initial_steps}\")\n",
    "\n",
    "\n",
    "for step_idx, exp in enumerate(exp_collector.rollout()):\n",
    "    # exp = exp_collector.rollout()\n",
    "    # print(exp)\n",
    "    print()\n",
    "    # if exp is None:\n",
    "    #     continue\n",
    "    # if step_idx>1:\n",
    "    #     break\n",
    "    # else:\n",
    "    #     continue\n",
    "    if len(exp['ep_rewards'])>0 :\n",
    "        for ep_rew in exp['ep_rewards']:\n",
    "            # Update Beta / Logger for EACH episode found\n",
    "            current_beta = beta_scheduler.update(ep_rew)\n",
    "            total_rewards.append(ep_rew)\n",
    "            mean_reward = float(np.mean(total_rewards[-100:]))\n",
    "            \n",
    "            print(f\"Episode: {episode_idx} |Steps: {step_idx} | Reward: {ep_rew} | Mean: {mean_reward:.1f}\")\n",
    "            \n",
    "            # wandb.log({\n",
    "            #     \"episode_reward\": ep_rew, \n",
    "            #     \"mean_reward_100\": mean_reward,\n",
    "            #     \"episode_number\": episode_idx\n",
    "            # }, step=step_idx)\n",
    "            \n",
    "            episode_idx += 1\n",
    "            \n",
    "            \n",
    "        \n",
    "            if mean_reward>950:\n",
    "                # save_path = os.path.join(wandb.run.dir, \"policy_best.pt\")\n",
    "                # torch.save(policy.state_dict(), save_path)\n",
    "                # wandb.log({\"best_policy_path\": save_path}, step=step_idx)\n",
    "                # print(f\"Solved! Mean reward > 950 at episode {episode_idx}\")\n",
    "                solved = True\n",
    "                break\n",
    "        if solved: \n",
    "            break    \n",
    "    \n",
    "    batch_states_t = torch.stack(exp['states'], dim=0)\n",
    "    batch_actions_t = torch.stack(exp['actions'], dim=0)\n",
    "    batch_adv= exp['adv']\n",
    "    batch_returns = exp['returns']\n",
    "\n",
    "    mu_new, std, value = policy(batch_states_t)\n",
    "    # print(value)\n",
    "    value_t = value.squeeze(dim=-1)\n",
    "    # print('values',value_t)\n",
    "    # break\n",
    "    \n",
    "    # loss_value = F.mse_loss(value_t, returns.detach())\n",
    "    #huberloss\n",
    "    delta = 1.0\n",
    "    loss_value = F.smooth_l1_loss(value_t,batch_returns.detach(), beta=delta)\n",
    "      \n",
    "    \n",
    "    dist_t = torch.distributions.Normal(mu_new, std)\n",
    "    logp_u = dist_t.log_prob(batch_actions_t).sum(dim=-1)\n",
    "    a_t = torch.tanh(batch_actions_t)\n",
    "    logp_correction = torch.log(( 1 - a_t.pow(2))+1e-6).sum(dim=-1)\n",
    "    logp = logp_u - logp_correction\n",
    "    \n",
    "    \n",
    "    batch_adv = (batch_adv - batch_adv.mean())/(batch_adv.std() + 1e-8) # normalize adv_t after returns\n",
    "\n",
    "    loss_policy = -(logp * batch_adv.detach()).mean()\n",
    "    \n",
    "    \n",
    "    \n",
    "    entropy = dist_t.entropy().sum(dim=-1).mean()\n",
    "    \n",
    "    loss_total = loss_value + loss_policy - current_beta*entropy\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_total.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)\n",
    "    optimizer.step()\n",
    "    scheduler.step(mean_reward)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        mu_t, std_t, v_t = policy(batch_states_t)\n",
    "        new_dist_t = torch.distributions.Normal(mu_t, std_t)\n",
    "        \n",
    "        kl_div = torch.distributions.kl_divergence(dist_t, new_dist_t).mean()\n",
    "        \n",
    "    grad_max = 0.0\n",
    "    grad_means = 0.0\n",
    "    grad_count = 0\n",
    "    for p in policy.parameters():\n",
    "\n",
    "        grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "        grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "        grad_count += 1\n",
    "        \n",
    "        \n",
    "    adv_smoothed = smooth(\n",
    "                    adv_smoothed,\n",
    "                    float(np.mean(batch_adv.abs().mean().item()))\n",
    "                )\n",
    "    l_entropy = smooth(l_entropy, entropy.item())\n",
    "    l_policy = smooth(l_policy, loss_policy.item())\n",
    "    l_value = smooth(l_value, loss_value.item())\n",
    "    l_total = smooth(l_total, loss_total.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # break\n",
    "    # # print(f\"Episode: {episode_idx} |Steps: {step_idx} | Reward: {ep_rew} | Mean: {mean_reward:.1f}\")\n",
    "    # wandb.log({\n",
    "    #     # 'baseline':baseline,\n",
    "    #     'entropy_beta':current_beta,\n",
    "    #     'advantage':adv_smoothed,\n",
    "    #     'entropy':entropy,\n",
    "    #     'loss_policy':l_policy,\n",
    "    #     'loss_value':l_value,\n",
    "    #     'loss_entropy': l_entropy, \n",
    "    #     'loss_total': l_total,\n",
    "    #     'kl div': kl_div.item(),\n",
    "    #     \"mu_delta\": (mu_new - mu_old).abs().mean().item(),\n",
    "    #     \"std\": std.mean().item(),\n",
    "    #     \"adv_abs\": batch_adv_t.abs().mean().item(),\n",
    "    #     'grad_l2':grad_means/grad_count if grad_count else 0.0,\n",
    "    #     'grad_max':grad_max,\n",
    "    #     'batch_returns': returns,\n",
    "    #     \"current_episode\": episode_idx, \n",
    "    #     'saturation_fractions':(a_t.abs() > 0.99).float().mean().item(),\n",
    "    #     'action_mean': batch_actions_t.mean().item(),\n",
    "    #     'action_std': batch_actions_t.std().item(),\n",
    "    #     'action_clamp_rate': (\n",
    "    #         ((batch_actions_t <= action_low + 0.01).any(dim=-1) | \n",
    "    #         (batch_actions_t >= action_high - 0.01).any(dim=-1))\n",
    "    #         .float().mean().item()\n",
    "    #     ),\n",
    "    #     'mu_mean': mu_new.mean().item(),\n",
    "    #     'mu_std': mu_new.std().item(),\n",
    "    #     'policy_std_mean': std.mean().item(),\n",
    "    # }, step = step_idx)\n",
    "    \n",
    "    # # batch_raw_actions.clear()\n",
    "    # # batch_returns.clear()\n",
    "    # # batch_states.clear()\n",
    "    # mu_old = mu_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bab34bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00233244 -0.00373462 -0.00850175 -0.00153787]\n",
      " [-0.00755476 -0.00759368  0.00877398  0.00321834]\n",
      " [ 0.00169804  0.00408513  0.00831098  0.00715005]]\n",
      "tensor([[ 0.0023, -0.0037, -0.0085, -0.0015],\n",
      "        [-0.0076, -0.0076,  0.0088,  0.0032],\n",
      "        [ 0.0017,  0.0041,  0.0083,  0.0072]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "state, _ = envs.reset()\n",
    "print(state)\n",
    "state_t = torch.tensor(state, dtype=torch.float32, device=device)#.unsqueeze(0)\n",
    "print(state_t)\n",
    "mu, std, val = policy(state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf12c153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNet(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (mu): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (critic_head): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b36d7da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-3.0, 3.0, (16, 1), float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52525cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (16, 4), float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8af06376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Box.sample of Box(-3.0, 3.0, (16, 1), float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.action_space.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e952aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Box.sample of Box(-inf, inf, (16, 4), float64)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.observation_space.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5adc7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
